142	0	8	Majority
142	9	16	assigns
142	21	39	sentiment polarity
142	53	72	largest probability
142	73	75	in
142	80	92	training set
142	99	109	Simple SVM
142	110	112	is
142	115	129	SVM classifier
142	130	134	with
142	135	150	simple features
142	151	158	such as
142	159	179	unigrams and bigrams
142	186	208	Feature - enhanced SVM
142	110	112	is
142	115	129	SVM classifier
142	130	134	with
142	236	275	state - of - the - art feature template
142	276	290	which contains
142	291	306	n-gram features
142	309	323	parse features
142	328	344	lexicon features
142	438	447	TD - LSTM
142	448	454	adopts
142	455	464	two LSTMs
142	465	473	to model
142	478	490	left context
142	130	134	with
142	496	502	target
142	511	524	right context
142	130	134	with
142	496	502	target
144	4	13	AE - LSTM
144	14	16	is
144	20	36	upgraded version
144	37	39	of
144	9	13	LSTM
147	4	15	ATAE - LSTM
147	19	37	developed based on
147	6	15	AE - LSTM
149	4	13	GRNN - G3
149	14	20	adopts
149	23	34	Gated - RNN
149	35	47	to represent
149	48	56	sentence
149	61	64	use
149	67	88	three - way structure
149	89	100	to leverage
149	101	109	contexts
151	0	6	MemNet
151	7	9	is
151	12	31	deep memory network
151	38	47	considers
151	52	72	content and position
151	73	75	of
151	76	82	target
153	0	3	IAN
153	4	24	interactively learns
153	25	35	attentions
153	4	6	in
153	43	63	contexts and targets
153	70	78	generate
153	83	98	representations
153	99	102	for
153	103	123	targets and contexts
131	18	27	dimension
131	28	30	of
131	31	53	word embedding vectors
131	58	78	hidden state vectors
131	79	81	is
131	82	85	300
133	0	48	All out - ofvocabulary words and weight matrices
133	53	76	randomly initialized by
133	79	117	uniform distribution U ( - 0.1 , 0.1 )
133	124	132	all bias
133	137	143	set to
133	144	148	zero
135	13	29	for implementing
135	34	54	neural network model
138	4	18	paired t- test
138	22	30	used for
138	35	55	significance testing
132	3	6	use
132	7	22	GloVe 2 vectors
132	23	27	with
132	28	42	300 dimensions
132	43	56	to initialize
132	61	76	word embeddings
136	0	2	In
136	3	17	model training
136	24	37	learning rate
136	41	47	set to
136	48	51	0.1
136	58	64	weight
136	65	68	for
136	69	94	L 2 - norm regularization
136	41	47	set to
136	105	112	1 e - 5
136	119	131	dropout rate
136	41	47	set to
136	142	145	0.5
137	3	8	train
137	13	18	model
137	19	22	use
137	23	60	stochastic gradient descent optimizer
137	61	65	with
137	66	74	momentum
137	75	77	of
137	78	81	0.9
38	71	78	propose
38	81	127	left - center - right separated neural network
38	128	132	with
38	133	175	rotatory attention mechanism ( LCR - Rot )
40	37	65	rotatory attention mechanism
40	69	86	take into account
40	91	102	interaction
40	103	110	between
40	111	131	targets and contexts
40	132	151	to better represent
40	111	131	targets and contexts
39	18	24	design
39	27	64	left - center - right separated LSTMs
39	70	78	contains
39	79	90	three LSTMs
39	93	97	i.e.
39	100	134	left - , center - and right - LSTM
39	150	158	modeling
39	163	174	three parts
39	175	177	of
39	180	186	review
39	189	201	left context
39	204	217	target phrase
39	222	235	right context
41	4	28	target2context attention
41	37	47	to capture
41	52	83	most indicative sentiment words
41	57	59	in
41	87	108	left / right contexts
42	19	43	context2target attention
42	52	62	to capture
42	67	86	most important word
42	87	89	in
42	27	33	target
43	5	13	leads to
43	16	41	two - side representation
43	42	44	of
43	49	55	target
43	58	77	left - aware target
43	82	102	right - aware target
44	13	24	concatenate
44	29	54	component representations
44	55	57	as
44	62	82	final representation
44	83	85	of
44	90	98	sentence
44	103	115	feed it into
44	118	131	softmax layer
44	132	142	to predict
44	147	165	sentiment polarity
2	51	84	Aspect - based Sentiment Analysis
16	15	33	sentiment analysis
17	54	78	sentiment classification
156	7	11	find
156	21	36	Majority method
156	37	39	is
156	44	49	worst
156	68	95	majority sentiment polarity
156	96	104	occupies
156	105	131	53.50 % , 65.00 % and 50 %
156	147	149	on
156	154	202	Restaurant , Laptop and Twitter testing datasets
157	4	20	Simple SVM model
157	21	29	performs
157	30	36	better
157	37	41	than
157	42	50	Majority
160	0	9	Our model
160	10	18	achieves
160	19	47	significantly better results
160	48	52	than
160	53	75	feature - enhanced SVM
162	63	82	basic LSTM approach
162	83	91	performs
162	96	101	worst
163	0	9	TD - LSTM
163	10	17	obtains
163	21	32	improvement
163	33	35	of
163	36	43	1 - 2 %
163	44	48	over
163	5	9	LSTM
163	54	58	when
163	59	73	target signals
163	78	88	taken into
163	89	102	consideration
166	0	6	MemNet
166	7	15	achieves
166	16	30	better results
166	31	35	than
166	36	48	other models
166	49	51	on
166	56	74	Restaurant dataset
167	0	3	IAN
167	4	13	considers
167	14	38	separate representations
167	39	41	of
167	42	49	targets
167	54	61	obtains
167	5	7	on
167	83	97	Laptop dataset
168	0	9	GRNN - G3
168	10	18	achieves
168	19	38	competitive results
168	39	41	on
168	42	54	all datasets
169	22	37	LCR - Rot model
169	38	46	achieves
169	51	63	best results
169	8	10	on
169	71	83	all datasets
169	84	89	among
169	90	100	all models
158	80	94	better results
158	0	16	With the help of
158	17	36	feature engineering
158	43	65	Feature - enhanced SVM
158	66	74	achieves
158	75	94	much better results
