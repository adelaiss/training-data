116	16	25	transfers
116	26	28	of
116	33	57	LSTM and embedding layer
116	58	61	are
116	62	73	more useful
116	74	78	than
116	83	95	output layer
118	6	37	Transfer of the embedding layer
118	38	40	is
118	41	53	more helpful
118	54	56	on
118	57	66	D3 and D4
120	0	21	Sentiment information
120	25	51	not adequately captured by
120	52	74	Glo Ve word embeddings
88	21	51	300 - dimension Glo Ve vectors
88	61	74	to initialize
88	75	82	E and E
88	83	87	when
88	88	99	pretraining
88	103	120	not conducted for
88	121	142	weight initialization
89	6	13	vectors
89	23	31	used for
89	32	46	initializing E
89	32	34	in
89	54	71	pretraining phase
91	3	18	randomly sample
91	19	23	20 %
91	24	26	of
91	31	53	original training data
91	54	58	from
91	63	82	aspectlevel dataset
91	63	65	as
91	90	105	development set
92	0	3	For
92	4	19	all experiments
92	26	35	dimension
92	36	38	of
92	39	58	LSTM hidden vectors
92	62	68	set to
92	69	72	300
93	23	26	use
93	27	34	dropout
93	35	39	with
93	40	55	probability 0.5
93	56	58	on
93	59	94	sentence / document representations
93	95	101	before
93	106	118	output layer
94	7	14	RMSProp
94	15	17	as
94	22	31	optimizer
94	32	36	with
94	41	51	decay rate
94	52	58	set to
94	59	62	0.9
94	71	89	base learning rate
94	52	58	set to
94	97	102	0.001
95	4	21	mini - batch size
95	25	31	set to
95	32	34	32
21	18	25	explore
21	26	46	two transfer methods
21	47	61	to incorporate
21	75	84	knowledge
21	87	98	pretraining
21	103	122	multi-task learning
2	34	73	Aspect - level Sentiment Classification
101	3	15	observe that
101	16	20	PRET
101	21	23	is
101	24	36	very helpful
101	43	61	consistently gives
101	64	80	1 - 3 % increase
101	72	74	in
101	84	92	accuracy
101	93	97	over
101	98	108	LSTM + ATT
103	0	4	MULT
103	5	10	gives
103	11	30	similar performance
103	31	33	as
103	34	44	LSTM + ATT
103	45	47	on
103	48	57	D1 and D2
104	4	31	combination ( PRET + MULT )
104	40	46	yields
104	47	61	better results
106	10	37	numbers of neutral examples
106	38	40	in
106	45	54	test sets
106	18	20	of
106	58	67	D3 and D4
106	68	71	are
106	72	82	very small
